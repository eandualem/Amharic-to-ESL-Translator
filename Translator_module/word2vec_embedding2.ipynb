{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"word2vec_embedding2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SgfzPzLx9unC","colab_type":"code","colab":{}},"source":["#from colab word2vec updated\n","import time\n","import numpy as np\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import preprocess_util\n","from collections import Counter\n","import random\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sPIcn09kOUx","colab_type":"code","colab":{}},"source":["!ln -s ./drive/My\\ Drive/HornMorph/\n","!cp ./drive/My\\ Drive/Amh2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XmHxMKPkQMk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2j7XOtKG9unI","colab_type":"code","colab":{}},"source":["RAW_DATA_FLD = \"./raw_data\"\n","RAW_DATA_FILE = \"amharic.txt\"\n","CLEANED_DATA_SAVE_PATH = \"./processed_data/cleaned_amh_text.txt\"\n","\n","\n","DATA_SRC_HIS = RAW_DATA_FLD + \"/Amharic_English_History/\" + RAW_DATA_FILE\n","DATA_SRC_NEWS = RAW_DATA_FLD + \"/Amharic_English_News/\" + RAW_DATA_FILE\n","DATA_SRC_LAW = RAW_DATA_FLD + \"/Amharic_English_Legal/\" + RAW_DATA_FILE\n","DATA_SRC_ETH_BIBLE = RAW_DATA_FLD + \"/Amharic_English_Ethiopic_Bible/\" + RAW_DATA_FILE\n","DATA_SRC_JW_BIBLW = RAW_DATA_FLD + \"/Amharic_English_JW_Bible/\" + RAW_DATA_FILE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxUT_SI_9unL","colab_type":"code","colab":{}},"source":["with open(DATA_SRC_HIS) as f:\n","    text_his = f.read()\n","\n","with open(DATA_SRC_NEWS) as f:\n","    text_news = f.read()\n","\n","with open(DATA_SRC_LAW) as f:\n","    text_law = f.read()\n","\n","with open(DATA_SRC_ETH_BIBLE) as f:\n","    text_ebible = f.read()\n","\n","with open(DATA_SRC_ETH_BIBLE) as f:\n","    text_jw_bible = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-moLe9c9unO","colab_type":"code","colab":{}},"source":["raw_text = text_his + \" \" + text_news + \" \" + text_law + \" \" + text_ebible + \" \" + text_jw_bible"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e1IFXrq9unR","colab_type":"code","colab":{}},"source":["cleaned_text = preprocess_util.clean_data(raw_text, save=True, savePath=CLEANED_DATA_SAVE_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gG-38rk99unU","colab_type":"code","colab":{}},"source":["#checkpoin1 cleaned data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvdxuyhR9unX","colab_type":"code","outputId":"7e714d2a-368d-4bd9-827c-47342fcadeff","executionInfo":{"status":"ok","timestamp":1590505147454,"user_tz":-180,"elapsed":4318,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import word2vec_helper as helper\n","import time\n","import numpy as np\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import preprocess_util\n","from collections import Counter\n","import random"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n",">>>>> This is L3Morpho, version 3.0 <<<<<\n",">>>>>  and HornMorpho, version 2.5  <<<<<\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fw5pm1xG9una","colab_type":"code","colab":{}},"source":["with open(CLEANED_DATA_SAVE_PATH) as f:\n","    cleaned_text = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2Qs374t9und","colab_type":"code","colab":{}},"source":["tokenized_text, lemmatized_tokenized_text = helper.tokenize(cleaned_text, 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8NdO5m09ung","colab_type":"code","outputId":"9a1ff550-c320-4fca-9b46-d6a90b0401cc","executionInfo":{"status":"ok","timestamp":1590505202533,"user_tz":-180,"elapsed":1477,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["preprocess_util.save_dump(\"./amh_lemmatized_tokens.p\", lemmatized_tokenized_text)\n","print(\"lemmatized_token_saved...\")\n","#checkpoint 2"],"execution_count":0,"outputs":[{"output_type":"stream","text":["lemmatized_token_saved...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y23BGuGu9unj","colab_type":"code","colab":{}},"source":["lemmatized_tokenized_text = preprocess_util.load_file(\"./amh_lemmatized_tokens.p\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsl2Ldk09unl","colab_type":"code","colab":{}},"source":["vocab_to_int, int_to_vocab = helper.create_lookup_tables(lemmatized_tokenized_text)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHKrszRA9uno","colab_type":"code","colab":{}},"source":["int_words = [vocab_to_int[word] for word in lemmatized_tokenized_text]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmfgdrdP9unt","colab_type":"code","colab":{}},"source":["threshold = 1e-5\n","word_counts = Counter(int_words)\n","total_count = len(int_words)\n","freqs = {word: count/total_count for word, count in word_counts.items()}\n","p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n","train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjEZ2qj79unx","colab_type":"code","colab":{}},"source":["def get_target(words, idx, window_size=5):\n","    ''' Get a list of words in a window around an index. '''\n","    \n","    R = np.random.randint(1, window_size+1)\n","    start = idx - R if (idx - R) > 0 else 0\n","    stop = idx + R\n","    target_words = set(words[start:idx] + words[idx+1:stop+1])\n","    \n","    return list(target_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZY9wxmX9unz","colab_type":"code","colab":{}},"source":["def get_batches(words, batch_size, window_size=5):\n","    n_batches = len(words)//batch_size\n","    words = words[:n_batches*batch_size]\n","    \n","    for idx in range(0, len(words), batch_size):\n","        x, y = [], []\n","        batch = words[idx:idx+batch_size]\n","        for ii in range(len(batch)):\n","            batch_x = batch[ii]\n","            batch_y = get_target(batch, ii, window_size)\n","            y.extend(batch_y)\n","            x.extend([batch_x]*len(batch_y))\n","        yield x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"693FPHuL9un1","colab_type":"code","colab":{}},"source":["train_graph = tf.Graph()\n","with train_graph.as_default():\n","    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n","    labels = tf.placeholder(tf.int32, [None, None], name='labels')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xb9ibLMV9un5","colab_type":"code","colab":{}},"source":["n_vocab = len(int_to_vocab)\n","n_embedding =  256\n","with train_graph.as_default():\n","    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n","    embed = tf.nn.embedding_lookup(embedding, inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ks0Esr0G9un7","colab_type":"code","colab":{}},"source":["n_sampled = 100\n","with train_graph.as_default():\n","    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n","    softmax_b = tf.zeros(n_vocab)\n","    \n","    # Calculate the loss using negative sampling\n","    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n","                                      labels, embed,\n","                                      n_sampled, n_vocab) \n","    \n","    cost = tf.reduce_mean(loss)\n","    optimizer = tf.train.AdamOptimizer().minimize(cost)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-F5ELHk9un-","colab_type":"code","colab":{}},"source":["with train_graph.as_default():\n","    ## From Thushan Ganegedara's implementation\n","    valid_size = 16 # Random set of words to evaluate similarity on.\n","    valid_window = 100\n","    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n","    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n","    valid_examples = np.append(valid_examples, \n","                               random.sample(range(1000,1000+valid_window), valid_size//2))\n","\n","    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n","    \n","    # We use the cosine distance:\n","    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n","    normalized_embedding = embedding / norm\n","    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n","    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7I-8t-kX9uoA","colab_type":"code","outputId":"ab0c710c-a4da-47a4-e19f-0c928ed88e16","executionInfo":{"status":"ok","timestamp":1590505745767,"user_tz":-180,"elapsed":7929,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!mkdir embedding_checkpoints"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘embedding_checkpoints’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kdqjgyHN9uoC","colab_type":"code","colab":{}},"source":["epochs = 100\n","batch_size = 2000\n","window_size = 10\n","\n","with train_graph.as_default():\n","    saver = tf.train.Saver()\n","\n","with tf.Session(graph=train_graph) as sess:\n","    iteration = 1\n","    loss = 0\n","    sess.run(tf.global_variables_initializer())\n","\n","    for e in range(1, epochs+1):\n","        batches = get_batches(train_words, batch_size, window_size)\n","        start = time.time()\n","        for x, y in batches:\n","            \n","            feed = {inputs: x,\n","                    labels: np.array(y)[:, None]}\n","            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n","            \n","            loss += train_loss\n","            \n","            if iteration % 100 == 0: \n","                end = time.time()\n","                print(\"Epoch {}/{}\".format(e, epochs),\n","                      \"Iteration: {}\".format(iteration),\n","                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n","                      \"{:.4f} sec/batch\".format((end-start)/100))\n","                loss = 0\n","                start = time.time()\n","            \n","            if iteration % 1000 == 0:\n","                ## From Thushan Ganegedara's implementation\n","                # note that this is expensive (~20% slowdown if computed every 500 steps)\n","                sim = similarity.eval()\n","                for i in range(valid_size):\n","                    valid_word = int_to_vocab[valid_examples[i]]\n","                    top_k = 8 # number of nearest neighbors\n","                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n","                    log = 'Nearest to %s:' % valid_word\n","                    for k in range(top_k):\n","                        close_word = int_to_vocab[nearest[k]]\n","                        log = '%s %s,' % (log, close_word)\n","                    print(log)\n","            \n","            iteration += 1\n","        save_path = saver.save(sess, \"embedding_checkpoints/amh_emebdding.ckpt\")\n","    embed_mat = sess.run(normalized_embedding)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3b7mkmp9uoF","colab_type":"code","colab":{}},"source":["preprocess_util.save_dump(\"./amh_vocab_mapping.p\",(int_to_vocab, vocab_to_int))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmTSatHe9uoH","colab_type":"code","outputId":"27721572-6271-4ee6-9a4f-e16d7608fbae","executionInfo":{"status":"ok","timestamp":1590507085320,"user_tz":-180,"elapsed":2038,"user":{"displayName":"daniel zelalem","photoUrl":"","userId":"02433931898077353749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with train_graph.as_default():\n","    saver = tf.train.Saver()\n","with tf.Session(graph=train_graph) as sess:\n","    saver.restore(sess, tf.train.latest_checkpoint('embedding_checkpoints'))\n","    embed_mat = sess.run(embedding)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from embedding_checkpoints/amh_emebdding.ckpt\n"],"name":"stdout"}]}]}